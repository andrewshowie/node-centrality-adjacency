{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZdkkM5Wp3CQ"
      },
      "outputs": [],
      "source": [
        "# ==== Install required packages ====\n",
        "!pip install fasttext\n",
        "!pip install igraph\n",
        "\n",
        "# ==== Imports ====\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from tqdm.auto import tqdm\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import concurrent.futures\n",
        "import time\n",
        "import traceback\n",
        "import gc\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "import igraph as ig\n",
        "import fasttext\n",
        "\n",
        "# ==== Silence warnings ====\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"networkx\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"networkx\")\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration and Settings\n",
        "# =============================================================================\n",
        "\n",
        "# File paths\n",
        "PLANCHUELO_CSV_PATH = \"/content/drive/MyDrive/cues_and_associates.csv\"\n",
        "FASTTEXT_EN_MODEL_PATH = \"/content/drive/MyDrive/cc.en.300.bin\"\n",
        "FASTTEXT_ES_MODEL_PATH = \"/content/drive/MyDrive/cc.es.300.bin\"\n",
        "OUTPUT_DIR = \"fasttext_network_analysis_v5_with_associates\"\n",
        "\n",
        "# Network generation parameters\n",
        "NUM_TOP_WORDS = 60000  # Number of top words to include in networks\n",
        "SIMILARITY_THRESHOLD = 0.4  # Threshold for similarity to create an edge\n",
        "\n",
        "# Automatically detect CPU cores\n",
        "N_WORKERS = max(1, multiprocessing.cpu_count() // 4)\n",
        "\n",
        "# =============================================================================\n",
        "# Utility Functions\n",
        "# =============================================================================\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Convert NumPy types to standard Python types for JSON serialization\"\"\"\n",
        "    if isinstance(obj, (np.float16, np.float32, np.float64)):\n",
        "        return float(obj) if not (np.isnan(obj) or np.isinf(obj)) else None\n",
        "    if isinstance(obj, (np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "        return int(obj)\n",
        "    if isinstance(obj, (np.complex64, np.complex128)):\n",
        "        return str(obj)\n",
        "    if isinstance(obj, (np.bool_)):\n",
        "        return bool(obj)\n",
        "    if isinstance(obj, (np.void)):\n",
        "        return None\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return [convert_numpy_types(i) for i in obj]\n",
        "    if isinstance(obj, dict):\n",
        "        return {convert_numpy_types(k): convert_numpy_types(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [convert_numpy_types(i) for i in obj]\n",
        "    if isinstance(obj, float) and (np.isnan(obj) or np.isinf(obj)):\n",
        "        return None\n",
        "    return obj\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalize text to handle different encodings and casing\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    try:\n",
        "        text_norm = ' '.join(text.strip().lower().split())\n",
        "        return unicodedata.normalize('NFC', text_norm)\n",
        "    except TypeError:\n",
        "        return text\n",
        "\n",
        "# =============================================================================\n",
        "# Data Loading and Processing\n",
        "# =============================================================================\n",
        "def load_planchuelo_data(file_path, log_messages):\n",
        "    \"\"\"Load Planchuelo et al. (2024) cue-associate data with participant information\"\"\"\n",
        "    log_messages.append(f\"Loading Planchuelo data from: {file_path}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Clean and normalize data\n",
        "    log_messages.append(\"Cleaning and normalizing Planchuelo data...\")\n",
        "    df.replace([\"\", \"NA\", \"No more responses\"], np.nan, inplace=True)\n",
        "    df.dropna(subset=['Language', 'Cue', 'Associate', 'Participant'], inplace=True)\n",
        "\n",
        "    # Normalize text fields\n",
        "    df['Language'] = df['Language'].astype(str).apply(normalize_text)\n",
        "    df['Cue'] = df['Cue'].astype(str).apply(normalize_text)\n",
        "    df['Associate'] = df['Associate'].astype(str).apply(normalize_text)\n",
        "    df['Participant'] = df['Participant'].astype(str)\n",
        "\n",
        "    # Extract all unique cue words and associate words by language\n",
        "    cue_words_by_lang = df.groupby('Language')['Cue'].apply(lambda x: set(x.unique())).to_dict()\n",
        "    associate_words_by_lang = df.groupby('Language')['Associate'].apply(lambda x: set(x.unique())).to_dict()\n",
        "\n",
        "    # Create a dataframe of cue-associate pairs with participant info\n",
        "    cue_associate_pairs = df[['Language', 'Participant', 'Cue', 'Associate']].copy()\n",
        "\n",
        "    log_messages.append(f\"Planchuelo data loaded successfully. (Time: {time.time() - start_time:.2f}s)\")\n",
        "    log_messages.append(f\"Cue words per language: {', '.join([f'{lang}: {len(words)}' for lang, words in cue_words_by_lang.items()])}\")\n",
        "    log_messages.append(f\"Associate words per language: {', '.join([f'{lang}: {len(words)}' for lang, words in associate_words_by_lang.items()])}\")\n",
        "    log_messages.append(f\"Total cue-associate pairs: {len(cue_associate_pairs)}\")\n",
        "\n",
        "    return cue_words_by_lang, associate_words_by_lang, cue_associate_pairs\n",
        "\n",
        "# =============================================================================\n",
        "# FastText Model and Word Embedding Processing\n",
        "# =============================================================================\n",
        "def load_fasttext_model(model_path, log_messages):\n",
        "    \"\"\"Load a FastText model from a file path\"\"\"\n",
        "    log_messages.append(f\"Loading FastText model from: {model_path}...\")\n",
        "    start_time = time.time()\n",
        "    model = fasttext.load_model(model_path)\n",
        "    log_messages.append(f\"FastText model loaded successfully. (Dimensions: {model.get_dimension()}) (Time: {time.time() - start_time:.2f}s)\")\n",
        "    return model\n",
        "\n",
        "def get_word_embeddings(ft_model, word_list, n_jobs=None, log_messages=None):\n",
        "    \"\"\"Process a list of words to get their embeddings using parallel processing\"\"\"\n",
        "    if log_messages is None:\n",
        "        log_messages = []\n",
        "    if n_jobs is None:\n",
        "        n_jobs = max(1, multiprocessing.cpu_count() // 2)\n",
        "\n",
        "    log_messages.append(f\"Extracting embeddings for {len(word_list)} words using {n_jobs} workers...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Deduplicate and convert to list if needed\n",
        "    if isinstance(word_list, set):\n",
        "        word_list = list(word_list)\n",
        "\n",
        "    # Process words in chunks for parallelization\n",
        "    chunk_size = max(1000, len(word_list) // (n_jobs * 2))\n",
        "    chunks = [word_list[i:i+chunk_size] for i in range(0, len(word_list), chunk_size)]\n",
        "\n",
        "    log_messages.append(f\"Processing {len(chunks)} chunks of words in parallel...\")\n",
        "\n",
        "    # Define the worker function to process each chunk\n",
        "    def process_chunk(chunk):\n",
        "        valid_words = []\n",
        "        embeddings = []\n",
        "        for word in chunk:\n",
        "            try:\n",
        "                vec = ft_model.get_word_vector(word)\n",
        "                embeddings.append(vec)\n",
        "                valid_words.append(word)\n",
        "            except Exception:\n",
        "                pass  # Skip problematic words\n",
        "        return valid_words, embeddings\n",
        "\n",
        "    # Process chunks in parallel\n",
        "    all_valid_words = []\n",
        "    all_embeddings = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
        "        future_to_chunk = {executor.submit(process_chunk, chunk): i for i, chunk in enumerate(chunks)}\n",
        "\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_chunk),\n",
        "                          total=len(chunks),\n",
        "                          desc=\"Fetching embeddings\",\n",
        "                          leave=False):\n",
        "            valid_words, embeddings = future.result()\n",
        "            all_valid_words.extend(valid_words)\n",
        "            all_embeddings.extend(embeddings)\n",
        "\n",
        "    # Convert to numpy array and normalize\n",
        "    embedding_matrix = np.array(all_embeddings, dtype=np.float32)\n",
        "\n",
        "    # L2 normalize for cosine similarity calculation\n",
        "    if embedding_matrix.size > 0:\n",
        "        from sklearn.preprocessing import normalize as sk_normalize\n",
        "        embedding_matrix = sk_normalize(embedding_matrix, norm='l2', axis=1)\n",
        "\n",
        "    log_messages.append(f\"Extracted embeddings for {len(all_valid_words)} words. (Time: {time.time() - start_time:.2f}s)\")\n",
        "    return all_valid_words, embedding_matrix\n",
        "\n",
        "def get_combined_word_embeddings(ft_model, top_n_count, custom_words=None, n_jobs=None, log_messages=None):\n",
        "    \"\"\"Get embeddings for both top frequency words and custom words (like cues and associates) in one pass\"\"\"\n",
        "    if log_messages is None:\n",
        "        log_messages = []\n",
        "    if n_jobs is None:\n",
        "        n_jobs = max(1, multiprocessing.cpu_count() // 2)\n",
        "\n",
        "    if custom_words is None:\n",
        "        custom_words = set()\n",
        "\n",
        "    log_messages.append(f\"Getting combined embeddings for top {top_n_count} words and {len(custom_words)} custom words...\")\n",
        "\n",
        "    # Get top frequency words from the model\n",
        "    top_words = ft_model.get_words()[:top_n_count]\n",
        "\n",
        "    # Combine with custom words, eliminating duplicates\n",
        "    combined_words = list(set(top_words) | set(custom_words))\n",
        "    log_messages.append(f\"Combined word list has {len(combined_words)} unique words\")\n",
        "\n",
        "    # Get all embeddings at once\n",
        "    valid_words, embeddings = get_word_embeddings(ft_model, combined_words, n_jobs, log_messages)\n",
        "\n",
        "    return valid_words, embeddings\n",
        "\n",
        "# =============================================================================\n",
        "# Similarity Matrix Calculation\n",
        "# =============================================================================\n",
        "def calculate_similarity_chunk(embeddings_matrix, threshold, start_idx, end_idx):\n",
        "    \"\"\"Calculate similarity matrix for a chunk of vectors - more efficiently\"\"\"\n",
        "    chunk_edges = []\n",
        "\n",
        "    # Process vectors in the chunk\n",
        "    for i in range(start_idx, end_idx):\n",
        "        # Calculate similarities with all vectors having higher indices\n",
        "        # This calculates only the upper triangle of the similarity matrix (efficient for undirected graphs)\n",
        "        similarities = np.dot(embeddings_matrix[i:i+1], embeddings_matrix[i+1:].T).flatten()\n",
        "\n",
        "        # Find edges that exceed the threshold\n",
        "        edges_idx = np.where(similarities > threshold)[0]\n",
        "\n",
        "        if edges_idx.size > 0:\n",
        "            # Create edges with similarity weights\n",
        "            for j in edges_idx:\n",
        "                chunk_edges.append((i, i+1+j, float(similarities[j])))\n",
        "\n",
        "    return chunk_edges\n",
        "\n",
        "def calculate_similarity_matrix(words, embeddings_matrix, threshold, n_jobs=None, log_messages=None):\n",
        "    \"\"\"Calculate similarity matrix in parallel for edge creation\"\"\"\n",
        "    if log_messages is None:\n",
        "        log_messages = []\n",
        "    if n_jobs is None:\n",
        "        n_jobs = max(1, multiprocessing.cpu_count() // 2)\n",
        "\n",
        "    n_vectors = embeddings_matrix.shape[0]\n",
        "    log_messages.append(f\"Calculating similarity matrix for {n_vectors} vectors using {n_jobs} workers...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create optimal chunk sizes based on vector count and available workers\n",
        "    chunk_size = max(100, min(1000, n_vectors // (n_jobs * 2)))\n",
        "    batches = [(i, min(i + chunk_size, n_vectors)) for i in range(0, n_vectors, chunk_size)]\n",
        "\n",
        "    log_messages.append(f\"Created {len(batches)} batches for similarity calculation\")\n",
        "\n",
        "    # Process batches in parallel\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
        "        futures = []\n",
        "        for batch_range in batches:\n",
        "            future = executor.submit(\n",
        "                calculate_similarity_chunk,\n",
        "                embeddings_matrix,\n",
        "                threshold,\n",
        "                *batch_range\n",
        "            )\n",
        "            futures.append(future)\n",
        "\n",
        "        # Collect results\n",
        "        edges_indices = []\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures),\n",
        "                         total=len(batches),\n",
        "                         desc=\"Calculating similarities\",\n",
        "                         leave=False):\n",
        "            edges_indices.extend(future.result())\n",
        "\n",
        "    # Convert indices to actual words with weights\n",
        "    log_messages.append(f\"Converting {len(edges_indices)} edge indices to word pairs...\")\n",
        "\n",
        "    edges_with_weights = [(words[s_idx], words[t_idx], weight)\n",
        "                         for s_idx, t_idx, weight in edges_indices]\n",
        "\n",
        "    log_messages.append(f\"Similarity calculation complete. Found {len(edges_with_weights)} edges. (Time: {time.time() - start_time:.2f}s)\")\n",
        "    return edges_with_weights\n",
        "\n",
        "# =============================================================================\n",
        "# Unified Network Construction\n",
        "# =============================================================================\n",
        "def build_semantic_network(words, embeddings_matrix, threshold, log_messages, n_jobs=None):\n",
        "    \"\"\"Build a semantic network from word embeddings in a single pass\"\"\"\n",
        "    log_messages.append(f\"Building semantic network (Nodes: {len(words)}, Threshold: {threshold})...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create graph and add nodes\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(words)\n",
        "\n",
        "    # Calculate edges using parallel processing\n",
        "    edges_with_weights = calculate_similarity_matrix(\n",
        "        words,\n",
        "        embeddings_matrix,\n",
        "        threshold,\n",
        "        n_jobs=n_jobs,\n",
        "        log_messages=log_messages\n",
        "    )\n",
        "\n",
        "    # Add edges to graph\n",
        "    log_messages.append(f\"Adding {len(edges_with_weights)} edges to the graph...\")\n",
        "    G.add_weighted_edges_from(edges_with_weights)\n",
        "\n",
        "    # Add distance attribute to edges (distance = 1 - similarity)\n",
        "    log_messages.append(f\"Adding 'distance' attribute to edges...\")\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        weight = data.get('weight', 0.0)\n",
        "        data['distance'] = 1.0 - weight\n",
        "\n",
        "    log_messages.append(f\"Built semantic network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges. (Time: {time.time() - start_time:.2f}s)\")\n",
        "    return G\n",
        "\n",
        "# =============================================================================\n",
        "# Unified Network Statistics Calculation\n",
        "# =============================================================================\n",
        "def calculate_centrality_metrics(G, log_messages, use_igraph=True):\n",
        "    \"\"\"Calculate centrality metrics for a graph using igraph\"\"\"\n",
        "    log_messages.append(f\"Calculating centrality metrics for graph with {G.number_of_nodes()} nodes...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Default dictionaries for metrics\n",
        "    metrics = {\n",
        "        'clustering': defaultdict(float),\n",
        "        'eigenvector': defaultdict(float),\n",
        "        'closeness': defaultdict(float),\n",
        "        'betweenness': defaultdict(float)\n",
        "    }\n",
        "\n",
        "    # Use igraph for computationally intensive calculations\n",
        "    if use_igraph:\n",
        "        log_messages.append(\"Using igraph for faster centrality calculations...\")\n",
        "\n",
        "        try:\n",
        "            # Convert NetworkX graph to igraph\n",
        "            g_ig = ig.Graph()\n",
        "            g_ig.add_vertices(list(G.nodes()))\n",
        "\n",
        "            # Map node names to indices\n",
        "            name_to_idx = {name: idx for idx, name in enumerate(G.nodes())}\n",
        "\n",
        "            # Add edges with weights\n",
        "            edges = [(name_to_idx[u], name_to_idx[v]) for u, v in G.edges()]\n",
        "            weights = [G[u][v].get('weight', 1.0) for u, v in G.edges()]\n",
        "            distances = [G[u][v].get('distance', 1.0) for u, v in G.edges()]\n",
        "\n",
        "            g_ig.add_edges(edges)\n",
        "            g_ig.es['weight'] = weights\n",
        "            g_ig.es['distance'] = distances\n",
        "\n",
        "            # Calculate centralities\n",
        "            clustering = g_ig.transitivity_local_undirected(weights='weight')\n",
        "            eigenvector = g_ig.eigenvector_centrality(weights='weight')\n",
        "            closeness = g_ig.closeness(weights='distance')\n",
        "            betweenness = g_ig.betweenness(weights='distance')\n",
        "\n",
        "            # Convert to dictionaries\n",
        "            node_names = list(G.nodes())\n",
        "            metrics['clustering'] = {name: score for name, score in zip(node_names, clustering)}\n",
        "            metrics['eigenvector'] = {name: score for name, score in zip(node_names, eigenvector)}\n",
        "            metrics['closeness'] = {name: score for name, score in zip(node_names, closeness)}\n",
        "            metrics['betweenness'] = {name: score for name, score in zip(node_names, betweenness)}\n",
        "\n",
        "            log_messages.append(f\"igraph centrality calculations complete. (Time: {time.time() - start_time:.2f}s)\")\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            log_messages.append(f\"Error using igraph: {e}. Falling back to NetworkX.\")\n",
        "\n",
        "    # If igraph not available or failed, use NetworkX\n",
        "    log_messages.append(\"Calculating centrality metrics with NetworkX...\")\n",
        "\n",
        "    # For very large graphs, sample betweenness\n",
        "    k_betweenness = min(1000, G.number_of_nodes() // 10) if G.number_of_nodes() > 30000 else None\n",
        "\n",
        "    # Calculate metrics\n",
        "    try:\n",
        "        metrics['clustering'] = nx.clustering(G, weight='weight')\n",
        "    except Exception as e:\n",
        "        log_messages.append(f\"Clustering calculation failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        metrics['eigenvector'] = nx.eigenvector_centrality_numpy(G, weight='weight', max_iter=1000, tol=1e-6)\n",
        "    except Exception as e:\n",
        "        log_messages.append(f\"Eigenvector centrality calculation failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        metrics['closeness'] = nx.closeness_centrality(G, distance='distance')\n",
        "    except Exception as e:\n",
        "        log_messages.append(f\"Closeness centrality calculation failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        metrics['betweenness'] = nx.betweenness_centrality(G, weight='distance', k=k_betweenness, normalized=True)\n",
        "    except Exception as e:\n",
        "        log_messages.append(f\"Betweenness centrality calculation failed: {e}\")\n",
        "\n",
        "    log_messages.append(f\"NetworkX centrality calculations complete. (Time: {time.time() - start_time:.2f}s)\")\n",
        "    return metrics\n",
        "\n",
        "def calculate_word_statistics(G, word_sets, language, log_messages):\n",
        "    \"\"\"\n",
        "    Calculate network statistics for different sets of words (cues and associates)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    G : NetworkX graph\n",
        "        The semantic network\n",
        "    word_sets : dict\n",
        "        Dictionary with keys as word types (e.g., 'Cue', 'Associate') and values as sets of words\n",
        "    language : str\n",
        "        Language code\n",
        "    log_messages : list\n",
        "        List to append log messages to\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary of word statistics by word type\n",
        "    \"\"\"\n",
        "    if not G or G.number_of_nodes() == 0:\n",
        "        log_messages.append(f\"Graph for {language} is empty. Cannot calculate statistics.\")\n",
        "        return {}\n",
        "\n",
        "    log_messages.append(f\"Calculating statistics for {sum(len(words) for words in word_sets.values())} words in {language} network...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Find the largest connected component (LCC)\n",
        "    log_messages.append(\"Finding connected components...\")\n",
        "    components = list(nx.connected_components(G))\n",
        "\n",
        "    if not components:\n",
        "        log_messages.append(\"No connected components found in graph.\")\n",
        "        return {}\n",
        "\n",
        "    largest_component = max(components, key=len)\n",
        "    G_lcc = G.subgraph(largest_component).copy()\n",
        "    log_messages.append(f\"Largest connected component has {G_lcc.number_of_nodes()} nodes and {G_lcc.number_of_edges()} edges\")\n",
        "\n",
        "    # Calculate centrality metrics for the LCC\n",
        "    centrality_metrics = calculate_centrality_metrics(G_lcc, log_messages)\n",
        "\n",
        "    # Prepare results for each word type\n",
        "    results = {}\n",
        "\n",
        "    for word_type, words in word_sets.items():\n",
        "        word_stats = []\n",
        "\n",
        "        for word in tqdm(words, desc=f\"Processing {word_type} words\", leave=False):\n",
        "            word_norm = normalize_text(str(word))\n",
        "            if pd.isna(word_norm):\n",
        "                continue\n",
        "\n",
        "            is_in_graph = word_norm in G\n",
        "            is_in_lcc = word_norm in largest_component\n",
        "\n",
        "            stats = {\n",
        "                'Word': word,\n",
        "                'Word_Type': word_type,\n",
        "                'Language': language,\n",
        "                'Clustering_Coefficient': centrality_metrics['clustering'].get(word_norm, 0.0) if is_in_lcc else 0.0,\n",
        "                'Eigenvector_Centrality': centrality_metrics['eigenvector'].get(word_norm, 0.0) if is_in_lcc else 0.0,\n",
        "                'Closeness_Centrality': centrality_metrics['closeness'].get(word_norm, 0.0) if is_in_lcc else 0.0,\n",
        "                'Betweenness_Centrality': centrality_metrics['betweenness'].get(word_norm, 0.0) if is_in_lcc else 0.0,\n",
        "                'Is_In_Graph': is_in_graph,\n",
        "                'Is_In_LCC': is_in_lcc,\n",
        "                'Degree': G.degree(word_norm, weight='weight') if is_in_graph else 0\n",
        "            }\n",
        "            word_stats.append(stats)\n",
        "\n",
        "        results[word_type] = word_stats\n",
        "\n",
        "    log_messages.append(f\"Statistics calculation complete. (Time: {time.time() - start_time:.2f}s)\")\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# Network Saving Functions\n",
        "# =============================================================================\n",
        "def save_network(G, language_code, output_dir):\n",
        "    \"\"\"Save network in multiple formats with error handling\"\"\"\n",
        "    # Create language-specific directory\n",
        "    lang_dir = os.path.join(output_dir, f\"network_{language_code}\")\n",
        "    os.makedirs(lang_dir, exist_ok=True)\n",
        "    print(f\"Saving network for {language_code} with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges...\")\n",
        "\n",
        "    paths = {}\n",
        "\n",
        "    try:\n",
        "        # Save as pickle\n",
        "        pickle_path = os.path.join(lang_dir, f\"{language_code}_network.pickle\")\n",
        "        with open(pickle_path, 'wb') as f:\n",
        "            pickle.dump(G, f)\n",
        "        paths['pickle'] = pickle_path\n",
        "\n",
        "        # Save largest connected component\n",
        "        components = list(nx.connected_components(G))\n",
        "        if components:\n",
        "            largest_component = max(components, key=len)\n",
        "            if len(largest_component) > 100:\n",
        "                G_lcc = G.subgraph(largest_component).copy()\n",
        "                lcc_pickle_path = os.path.join(lang_dir, f\"{language_code}_lcc_network.pickle\")\n",
        "                with open(lcc_pickle_path, 'wb') as f:\n",
        "                    pickle.dump(G_lcc, f)\n",
        "                paths['lcc_pickle'] = lcc_pickle_path\n",
        "\n",
        "        # Save network metadata\n",
        "        meta = {\n",
        "            'nodes': G.number_of_nodes(),\n",
        "            'edges': G.number_of_edges(),\n",
        "            'density': nx.density(G),\n",
        "            'components': len(components) if components else 0,\n",
        "            'largest_component_size': len(max(components, key=len)) if components else 0\n",
        "        }\n",
        "\n",
        "        meta_path = os.path.join(lang_dir, f\"{language_code}_network_meta.json\")\n",
        "        with open(meta_path, 'w') as f:\n",
        "            json.dump(meta, f, indent=2)\n",
        "        paths['meta'] = meta_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving network: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return paths\n",
        "\n",
        "def save_word_statistics(word_stats, language_code, output_dir):\n",
        "    \"\"\"Save word statistics to CSV files\"\"\"\n",
        "    stats_dir = os.path.join(output_dir, \"statistics\")\n",
        "    os.makedirs(stats_dir, exist_ok=True)\n",
        "\n",
        "    all_stats = []\n",
        "    for word_type, stats in word_stats.items():\n",
        "        # Append all stats to the combined list\n",
        "        all_stats.extend(stats)\n",
        "\n",
        "        # Save type-specific stats\n",
        "        type_stats_path = os.path.join(stats_dir, f\"{language_code}_{word_type.lower()}_statistics.csv\")\n",
        "        type_df = pd.DataFrame(stats)\n",
        "        type_df.to_csv(type_stats_path, index=False)\n",
        "        print(f\"Saved {word_type} statistics to {type_stats_path}\")\n",
        "\n",
        "    # Save combined stats\n",
        "    combined_stats_path = os.path.join(stats_dir, f\"{language_code}_all_word_statistics.csv\")\n",
        "    combined_df = pd.DataFrame(all_stats)\n",
        "    combined_df.to_csv(combined_stats_path, index=False)\n",
        "    print(f\"Saved combined statistics to {combined_stats_path}\")\n",
        "\n",
        "    return {\n",
        "        'combined': combined_stats_path,\n",
        "        'by_type': {word_type: os.path.join(stats_dir, f\"{language_code}_{word_type.lower()}_statistics.csv\")\n",
        "                   for word_type in word_stats.keys()}\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# Language Processing Pipeline\n",
        "# =============================================================================\n",
        "def process_language(language_code, ft_model_path, cue_words, associate_words, num_top_words,\n",
        "                    sim_threshold, output_dir, n_workers=None):\n",
        "    \"\"\"\n",
        "    Process a language end-to-end in a unified pipeline:\n",
        "    1. Load FastText model\n",
        "    2. Get embeddings for all words (top frequency + cues + associates)\n",
        "    3. Build a single semantic network with all words\n",
        "    4. Calculate network statistics for cue and associate words\n",
        "    5. Save network and statistics\n",
        "    \"\"\"\n",
        "    if n_workers is None:\n",
        "        n_workers = max(1, multiprocessing.cpu_count() // 2)\n",
        "\n",
        "    log_messages = []\n",
        "    log_messages.append(f\"\\n--- Starting Unified Pipeline for {language_code} ---\")\n",
        "\n",
        "    results = {\n",
        "        'network_paths': {},\n",
        "        'statistics_paths': {},\n",
        "        'word_stats': {},\n",
        "        'success': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # 1. Load FastText model\n",
        "        ft_model = load_fasttext_model(ft_model_path, log_messages)\n",
        "        if ft_model is None:\n",
        "            log_messages.append(f\"Failed to load FastText model for {language_code}. Aborting pipeline.\")\n",
        "            return results, log_messages\n",
        "\n",
        "        # 2. Get embeddings for all words at once\n",
        "        all_custom_words = set(cue_words) | set(associate_words)\n",
        "        all_words, embeddings = get_combined_word_embeddings(\n",
        "            ft_model,\n",
        "            num_top_words,\n",
        "            custom_words=all_custom_words,\n",
        "            n_jobs=n_workers,\n",
        "            log_messages=log_messages\n",
        "        )\n",
        "\n",
        "        # 3. Build semantic network\n",
        "        G = build_semantic_network(\n",
        "            all_words,\n",
        "            embeddings,\n",
        "            sim_threshold,\n",
        "            log_messages,\n",
        "            n_jobs=n_workers\n",
        "        )\n",
        "\n",
        "        # Free up memory\n",
        "        del embeddings\n",
        "        gc.collect()\n",
        "\n",
        "        # 4. Calculate statistics for different word types\n",
        "        word_sets = {\n",
        "            'Cue': set(cue_words),\n",
        "            'Associate': set(associate_words)\n",
        "        }\n",
        "\n",
        "        word_stats = calculate_word_statistics(\n",
        "            G,\n",
        "            word_sets,\n",
        "            language_code,\n",
        "            log_messages\n",
        "        )\n",
        "        results['word_stats'] = word_stats\n",
        "\n",
        "        # 5. Save network and statistics\n",
        "        if output_dir:\n",
        "            results['network_paths'] = save_network(G, language_code, output_dir)\n",
        "\n",
        "            if word_stats:\n",
        "                results['statistics_paths'] = save_word_statistics(word_stats, language_code, output_dir)\n",
        "\n",
        "        # Free memory\n",
        "        del ft_model\n",
        "        gc.collect()\n",
        "\n",
        "        results['success'] = True\n",
        "        log_messages.append(f\"--- Completed Unified Pipeline for {language_code} Successfully ---\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_messages.append(f\"Error in {language_code} pipeline: {e}\")\n",
        "        log_messages.append(traceback.format_exc())\n",
        "\n",
        "    return results, log_messages\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution Block\n",
        "# =============================================================================\n",
        "def main():\n",
        "    print(f\"CPU Count: {multiprocessing.cpu_count()}\")\n",
        "    print(f\"Using {N_WORKERS} worker processes\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "    # Load Planchuelo data\n",
        "    main_log = []\n",
        "    cue_words_by_lang, associate_words_by_lang, cue_associate_pairs = load_planchuelo_data(\n",
        "        PLANCHUELO_CSV_PATH, main_log\n",
        "    )\n",
        "\n",
        "    # Print setup info\n",
        "    for msg in main_log:\n",
        "        print(msg)\n",
        "\n",
        "    if not cue_words_by_lang:\n",
        "        print(\"Error: No cue words loaded. Cannot continue.\")\n",
        "        return\n",
        "\n",
        "    # Define language tasks\n",
        "    tasks = []\n",
        "\n",
        "    # English\n",
        "    if os.path.exists(FASTTEXT_EN_MODEL_PATH):\n",
        "        en_cues = cue_words_by_lang.get('english', set())\n",
        "        en_associates = associate_words_by_lang.get('english', set())\n",
        "\n",
        "        tasks.append({\n",
        "            'language_code': 'en',\n",
        "            'language_full': 'english',\n",
        "            'model_path': FASTTEXT_EN_MODEL_PATH,\n",
        "            'cue_words': en_cues,\n",
        "            'associate_words': en_associates\n",
        "        })\n",
        "    else:\n",
        "        print(f\"WARNING: English FastText model not found at {FASTTEXT_EN_MODEL_PATH}\")\n",
        "\n",
        "    # Spanish\n",
        "    if os.path.exists(FASTTEXT_ES_MODEL_PATH):\n",
        "        es_cues = cue_words_by_lang.get('spanish', set())\n",
        "        es_associates = associate_words_by_lang.get('spanish', set())\n",
        "\n",
        "        tasks.append({\n",
        "            'language_code': 'es',\n",
        "            'language_full': 'spanish',\n",
        "            'model_path': FASTTEXT_ES_MODEL_PATH,\n",
        "            'cue_words': es_cues,\n",
        "            'associate_words': es_associates\n",
        "        })\n",
        "    else:\n",
        "        print(f\"WARNING: Spanish FastText model not found at {FASTTEXT_ES_MODEL_PATH}\")\n",
        "\n",
        "    # Process languages\n",
        "    if not tasks:\n",
        "        print(\"No language tasks to process. Check FastText model paths.\")\n",
        "        return\n",
        "\n",
        "    # Results container\n",
        "    all_results = {}\n",
        "\n",
        "    # Process each language\n",
        "    for task in tasks:\n",
        "        print(f\"\\nProcessing {task['language_full']} language...\")\n",
        "\n",
        "        results, logs = process_language(\n",
        "            language_code=task['language_code'],\n",
        "            ft_model_path=task['model_path'],\n",
        "            cue_words=task['cue_words'],\n",
        "            associate_words=task['associate_words'],\n",
        "            num_top_words=NUM_TOP_WORDS,\n",
        "            sim_threshold=SIMILARITY_THRESHOLD,\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            n_workers=N_WORKERS\n",
        "        )\n",
        "\n",
        "        # Print logs\n",
        "        for log in logs:\n",
        "            print(log)\n",
        "\n",
        "        all_results[task['language_code']] = results\n",
        "\n",
        "    # Create final merged dataset with cue-associate pairs and network metrics\n",
        "    print(\"\\nCreating final cue-associate dataset with network metrics...\")\n",
        "\n",
        "    try:\n",
        "        # Start with the original cue-associate pairs\n",
        "        final_df = cue_associate_pairs.copy()\n",
        "\n",
        "        # Add language code mapping\n",
        "        language_code_map = {'english': 'en', 'spanish': 'es'}\n",
        "        final_df['Language_Code'] = final_df['Language'].map(language_code_map)\n",
        "\n",
        "        # Create empty columns for all metrics\n",
        "        metric_columns = [\n",
        "            'Cue_Clustering_Coefficient', 'Cue_Eigenvector_Centrality',\n",
        "            'Cue_Closeness_Centrality', 'Cue_Betweenness_Centrality',\n",
        "            'Cue_Is_In_Graph', 'Cue_Is_In_LCC', 'Cue_Degree',\n",
        "            'Associate_Clustering_Coefficient', 'Associate_Eigenvector_Centrality',\n",
        "            'Associate_Closeness_Centrality', 'Associate_Betweenness_Centrality',\n",
        "            'Associate_Is_In_Graph', 'Associate_Is_In_LCC', 'Associate_Degree'\n",
        "        ]\n",
        "\n",
        "        for col in metric_columns:\n",
        "            final_df[col] = np.nan\n",
        "\n",
        "        # Process languages one by one\n",
        "        for lang_code, lang_results in all_results.items():\n",
        "            if not lang_results['success'] or not lang_results['word_stats']:\n",
        "                print(f\"Skipping language {lang_code} - missing results\")\n",
        "                continue\n",
        "\n",
        "            # Get language full name (reverse map)\n",
        "            lang_full = next((k for k, v in language_code_map.items() if v == lang_code), None)\n",
        "            if not lang_full:\n",
        "                print(f\"Skipping language code {lang_code} - no matching language name\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing metrics for language: {lang_full}\")\n",
        "\n",
        "            # Create dictionaries to efficiently lookup metrics\n",
        "            cue_metrics = {}\n",
        "            associate_metrics = {}\n",
        "\n",
        "            # Process cue metrics\n",
        "            if 'Cue' in lang_results['word_stats']:\n",
        "                print(f\"Processing cue metrics for {lang_full}\")\n",
        "                for stat in lang_results['word_stats']['Cue']:\n",
        "                    word = stat['Word']\n",
        "                    word_norm = normalize_text(str(word))\n",
        "                    if pd.isna(word_norm):\n",
        "                        continue\n",
        "\n",
        "                    cue_metrics[word_norm] = {\n",
        "                        'Clustering_Coefficient': stat['Clustering_Coefficient'],\n",
        "                        'Eigenvector_Centrality': stat['Eigenvector_Centrality'],\n",
        "                        'Closeness_Centrality': stat['Closeness_Centrality'],\n",
        "                        'Betweenness_Centrality': stat['Betweenness_Centrality'],\n",
        "                        'Is_In_Graph': stat['Is_In_Graph'],\n",
        "                        'Is_In_LCC': stat['Is_In_LCC'],\n",
        "                        'Degree': stat['Degree']\n",
        "                    }\n",
        "\n",
        "            # Process associate metrics\n",
        "            if 'Associate' in lang_results['word_stats']:\n",
        "                print(f\"Processing associate metrics for {lang_full}\")\n",
        "                for stat in lang_results['word_stats']['Associate']:\n",
        "                    word = stat['Word']\n",
        "                    word_norm = normalize_text(str(word))\n",
        "                    if pd.isna(word_norm):\n",
        "                        continue\n",
        "\n",
        "                    associate_metrics[word_norm] = {\n",
        "                        'Clustering_Coefficient': stat['Clustering_Coefficient'],\n",
        "                        'Eigenvector_Centrality': stat['Eigenvector_Centrality'],\n",
        "                        'Closeness_Centrality': stat['Closeness_Centrality'],\n",
        "                        'Betweenness_Centrality': stat['Betweenness_Centrality'],\n",
        "                        'Is_In_Graph': stat['Is_In_Graph'],\n",
        "                        'Is_In_LCC': stat['Is_In_LCC'],\n",
        "                        'Degree': stat['Degree']\n",
        "                    }\n",
        "\n",
        "            # Apply metrics to the appropriate rows\n",
        "            lang_mask = final_df['Language'] == lang_full\n",
        "\n",
        "            # For each row in this language\n",
        "            for idx, row in final_df[lang_mask].iterrows():\n",
        "                cue_norm = normalize_text(str(row['Cue']))\n",
        "                associate_norm = normalize_text(str(row['Associate']))\n",
        "\n",
        "                # Apply cue metrics if available\n",
        "                if cue_norm in cue_metrics:\n",
        "                    metrics = cue_metrics[cue_norm]\n",
        "                    final_df.loc[idx, 'Cue_Clustering_Coefficient'] = metrics['Clustering_Coefficient']\n",
        "                    final_df.loc[idx, 'Cue_Eigenvector_Centrality'] = metrics['Eigenvector_Centrality']\n",
        "                    final_df.loc[idx, 'Cue_Closeness_Centrality'] = metrics['Closeness_Centrality']\n",
        "                    final_df.loc[idx, 'Cue_Betweenness_Centrality'] = metrics['Betweenness_Centrality']\n",
        "                    final_df.loc[idx, 'Cue_Is_In_Graph'] = metrics['Is_In_Graph']\n",
        "                    final_df.loc[idx, 'Cue_Is_In_LCC'] = metrics['Is_In_LCC']\n",
        "                    final_df.loc[idx, 'Cue_Degree'] = metrics['Degree']\n",
        "\n",
        "                # Apply associate metrics if available\n",
        "                if associate_norm in associate_metrics:\n",
        "                    metrics = associate_metrics[associate_norm]\n",
        "                    final_df.loc[idx, 'Associate_Clustering_Coefficient'] = metrics['Clustering_Coefficient']\n",
        "                    final_df.loc[idx, 'Associate_Eigenvector_Centrality'] = metrics['Eigenvector_Centrality']\n",
        "                    final_df.loc[idx, 'Associate_Closeness_Centrality'] = metrics['Closeness_Centrality']\n",
        "                    final_df.loc[idx, 'Associate_Betweenness_Centrality'] = metrics['Betweenness_Centrality']\n",
        "                    final_df.loc[idx, 'Associate_Is_In_Graph'] = metrics['Is_In_Graph']\n",
        "                    final_df.loc[idx, 'Associate_Is_In_LCC'] = metrics['Is_In_LCC']\n",
        "                    final_df.loc[idx, 'Associate_Degree'] = metrics['Degree']\n",
        "\n",
        "            print(f\"Applied metrics for {lang_full} language\")\n",
        "\n",
        "        # Save final dataset\n",
        "        final_path = os.path.join(OUTPUT_DIR, \"cue_associate_network_metrics.csv\")\n",
        "        final_df.to_csv(final_path, index=False)\n",
        "        print(f\"Saved final cue-associate network metrics to: {final_path}\")\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(\"\\nFinal Dataset Summary:\")\n",
        "        print(f\"Total cue-associate pairs: {len(final_df)}\")\n",
        "        print(f\"Pairs with cue metrics: {final_df['Cue_Clustering_Coefficient'].notna().sum()}\")\n",
        "        print(f\"Pairs with associate metrics: {final_df['Associate_Clustering_Coefficient'].notna().sum()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating final dataset: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\nNetwork analysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1RCuFAi96pBvOjoq0zNjDwPlb0Far1i6B",
      "authorship_tag": "ABX9TyM2sRpmf1uzwurFsLsTv+jU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}